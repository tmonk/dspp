{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary based text analysis in Python\n",
    "## Sentiment analysis\n",
    "**Thomas Monk**  \n",
    "**London School of Economics**  \n",
    "with thanks to Chris Bail, Duke University - converted to Pandas & Python from R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word-counting techniques and dictionary-based methods are the most simple forms of quantitative text analysis.\n",
    "\n",
    "This tutorial will cover both of these topics, as well as sentiment analysis, which is a form of dictionary-based text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal.\n",
    "\n",
    "I want you to output a single diagram that tells us the following: *did Donald Trump's tweets become more or less negative across his term in office*? \n",
    "\n",
    "Perhaps uninspired and outdated at this point! - but the data is so rich and easily accessible, and perfect for this kind of analysis. I want you to think about where this techniques could be useful to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data - Processed Tweets\n",
    "We've already processed the tweet data - now lets run our sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>isDeleted</th>\n",
       "      <th>device</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>isFlagged</th>\n",
       "      <th>textp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815422340540547073</td>\n",
       "      <td>TO ALL AMERICANS-#HappyNewYear &amp;amp, many bles...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>108920</td>\n",
       "      <td>26891</td>\n",
       "      <td>2017-01-01 05:00:10</td>\n",
       "      <td>f</td>\n",
       "      <td>many blessing all look forward wonderful prosp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815930688889352192</td>\n",
       "      <td>Well, the New Year begins. We will, together, ...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>105506</td>\n",
       "      <td>23739</td>\n",
       "      <td>2017-01-02 14:40:10</td>\n",
       "      <td>f</td>\n",
       "      <td>well new year begin will together make america...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815973752785793024</td>\n",
       "      <td>Chicago murder rate is record setting - 4,331 ...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>52993</td>\n",
       "      <td>13992</td>\n",
       "      <td>2017-01-02 17:31:17</td>\n",
       "      <td>f</td>\n",
       "      <td>chicago murder rate record set shoot victim mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815989154555297792</td>\n",
       "      <td>\"@CNN just released a book called \"\"Unpreceden...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>11394</td>\n",
       "      <td>3165</td>\n",
       "      <td>2017-01-02 18:32:29</td>\n",
       "      <td>f</td>\n",
       "      <td>just release book call unprecedented explores ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815990335318982656</td>\n",
       "      <td>Various media outlets and pundits say that I t...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>39567</td>\n",
       "      <td>7264</td>\n",
       "      <td>2017-01-02 18:37:10</td>\n",
       "      <td>f</td>\n",
       "      <td>various medium outlet pundit say thought go lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>1079763419908243456</td>\n",
       "      <td>I’m in the Oval Office. Democrats, come back f...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>126997</td>\n",
       "      <td>27021</td>\n",
       "      <td>2018-12-31 15:37:14</td>\n",
       "      <td>f</td>\n",
       "      <td>oval office democrat come back vacation now gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5392</th>\n",
       "      <td>1079763923845419009</td>\n",
       "      <td>It’s incredible how Democrats can all use thei...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>125636</td>\n",
       "      <td>26560</td>\n",
       "      <td>2018-12-31 15:39:15</td>\n",
       "      <td>f</td>\n",
       "      <td>it incredible democrat can use ridiculous soun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>1079830267274108930</td>\n",
       "      <td>Heads of countries are calling wanting to know...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>87357</td>\n",
       "      <td>21317</td>\n",
       "      <td>2018-12-31 20:02:52</td>\n",
       "      <td>f</td>\n",
       "      <td>head country call want know senator schumer ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>1079830268708556800</td>\n",
       "      <td>....Senator Schumer, more than a year longer t...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>75463</td>\n",
       "      <td>17875</td>\n",
       "      <td>2018-12-31 20:02:52</td>\n",
       "      <td>f</td>\n",
       "      <td>senator schumer year longer administration his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>1079888205351145472</td>\n",
       "      <td>HAPPY NEW YEAR! https://t.co/bHoPDPQ7G6</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>144692</td>\n",
       "      <td>31142</td>\n",
       "      <td>2018-12-31 23:53:06</td>\n",
       "      <td>f</td>\n",
       "      <td>happy new year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5396 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0      815422340540547073  TO ALL AMERICANS-#HappyNewYear &amp, many bles...   \n",
       "1      815930688889352192  Well, the New Year begins. We will, together, ...   \n",
       "2      815973752785793024  Chicago murder rate is record setting - 4,331 ...   \n",
       "3      815989154555297792  \"@CNN just released a book called \"\"Unpreceden...   \n",
       "4      815990335318982656  Various media outlets and pundits say that I t...   \n",
       "...                   ...                                                ...   \n",
       "5391  1079763419908243456  I’m in the Oval Office. Democrats, come back f...   \n",
       "5392  1079763923845419009  It’s incredible how Democrats can all use thei...   \n",
       "5393  1079830267274108930  Heads of countries are calling wanting to know...   \n",
       "5394  1079830268708556800  ....Senator Schumer, more than a year longer t...   \n",
       "5395  1079888205351145472            HAPPY NEW YEAR! https://t.co/bHoPDPQ7G6   \n",
       "\n",
       "     isRetweet isDeleted               device  favorites  retweets  \\\n",
       "0            f         f   Twitter for iPhone     108920     26891   \n",
       "1            f         f  Twitter for Android     105506     23739   \n",
       "2            f         f  Twitter for Android      52993     13992   \n",
       "3            f         f  Twitter for Android      11394      3165   \n",
       "4            f         f  Twitter for Android      39567      7264   \n",
       "...        ...       ...                  ...        ...       ...   \n",
       "5391         f         f   Twitter for iPhone     126997     27021   \n",
       "5392         f         f   Twitter for iPhone     125636     26560   \n",
       "5393         f         f   Twitter for iPhone      87357     21317   \n",
       "5394         f         f   Twitter for iPhone      75463     17875   \n",
       "5395         f         f   Twitter for iPhone     144692     31142   \n",
       "\n",
       "                     date isFlagged  \\\n",
       "0     2017-01-01 05:00:10         f   \n",
       "1     2017-01-02 14:40:10         f   \n",
       "2     2017-01-02 17:31:17         f   \n",
       "3     2017-01-02 18:32:29         f   \n",
       "4     2017-01-02 18:37:10         f   \n",
       "...                   ...       ...   \n",
       "5391  2018-12-31 15:37:14         f   \n",
       "5392  2018-12-31 15:39:15         f   \n",
       "5393  2018-12-31 20:02:52         f   \n",
       "5394  2018-12-31 20:02:52         f   \n",
       "5395  2018-12-31 23:53:06         f   \n",
       "\n",
       "                                                  textp  \n",
       "0     many blessing all look forward wonderful prosp...  \n",
       "1     well new year begin will together make america...  \n",
       "2     chicago murder rate record set shoot victim mu...  \n",
       "3     just release book call unprecedented explores ...  \n",
       "4     various medium outlet pundit say thought go lo...  \n",
       "...                                                 ...  \n",
       "5391  oval office democrat come back vacation now gi...  \n",
       "5392  it incredible democrat can use ridiculous soun...  \n",
       "5393  head country call want know senator schumer ap...  \n",
       "5394  senator schumer year longer administration his...  \n",
       "5395                                    happy new year   \n",
       "\n",
       "[5396 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('processed_tweets.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Document-Term Matrix\n",
    "\n",
    "A core concept in quantitative text analysis is a Document-Term Matrix. This is a matrix where each word is a row and each colum is a document. The number within each cell describes the number of times the word appears in the document. Many of the most popular forms of text analysis, such as topic models, require a document term matrix.\n",
    "\n",
    "We'll use the CountVectorizer package from sklearn here - be liberal with your package use, don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc 0</th>\n",
       "      <th>Doc 1</th>\n",
       "      <th>Doc 2</th>\n",
       "      <th>Doc 3</th>\n",
       "      <th>Doc 4</th>\n",
       "      <th>Doc 5</th>\n",
       "      <th>Doc 6</th>\n",
       "      <th>Doc 7</th>\n",
       "      <th>Doc 8</th>\n",
       "      <th>Doc 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Doc 5282</th>\n",
       "      <th>Doc 5283</th>\n",
       "      <th>Doc 5284</th>\n",
       "      <th>Doc 5285</th>\n",
       "      <th>Doc 5286</th>\n",
       "      <th>Doc 5287</th>\n",
       "      <th>Doc 5288</th>\n",
       "      <th>Doc 5289</th>\n",
       "      <th>Doc 5290</th>\n",
       "      <th>Doc 5291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbott</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zte</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuker</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>élysée</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6508 rows × 5292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Doc 0  Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  \\\n",
       "aaa          0      0      0      0      0      0      0      0      0      0   \n",
       "abandon      0      0      0      0      0      0      0      0      0      0   \n",
       "abbas        0      0      0      0      0      0      0      0      0      0   \n",
       "abbott       0      0      0      0      0      0      0      0      0      0   \n",
       "abc          0      0      0      0      0      0      0      0      0      0   \n",
       "...        ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "zone         0      0      0      0      0      0      0      0      0      0   \n",
       "zoo          0      0      0      0      0      0      0      0      0      0   \n",
       "zte          0      0      0      0      0      0      0      0      0      0   \n",
       "zuker        0      0      0      0      0      0      0      0      0      0   \n",
       "élysée       0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         ...  Doc 5282  Doc 5283  Doc 5284  Doc 5285  Doc 5286  Doc 5287  \\\n",
       "aaa      ...         0         0         0         0         0         0   \n",
       "abandon  ...         0         0         0         0         0         0   \n",
       "abbas    ...         0         0         0         0         0         0   \n",
       "abbott   ...         0         0         0         0         0         0   \n",
       "abc      ...         0         0         0         0         0         0   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "zone     ...         0         0         0         0         0         0   \n",
       "zoo      ...         0         0         0         0         0         0   \n",
       "zte      ...         0         0         0         0         0         0   \n",
       "zuker    ...         0         0         0         0         0         0   \n",
       "élysée   ...         0         0         0         0         0         0   \n",
       "\n",
       "         Doc 5288  Doc 5289  Doc 5290  Doc 5291  \n",
       "aaa             0         0         0         0  \n",
       "abandon         0         0         0         0  \n",
       "abbas           0         0         0         0  \n",
       "abbott          0         0         0         0  \n",
       "abc             0         0         0         0  \n",
       "...           ...       ...       ...       ...  \n",
       "zone            0         0         0         0  \n",
       "zoo             0         0         0         0  \n",
       "zte             0         0         0         0  \n",
       "zuker           0         0         0         0  \n",
       "élysée          0         0         0         0  \n",
       "\n",
       "[6508 rows x 5292 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Drop null values first\n",
    "df.dropna(subset=['textp'], inplace=True)\n",
    "\n",
    "# Fit the vectorizer to the data\n",
    "X = vectorizer.fit_transform(df['textp'])\n",
    "\n",
    "# Convert the X matrix to a dataframe\n",
    "dtm = pd.DataFrame(X.todense())\n",
    "\n",
    "# Save the length of the data for later on - this give us the number of docs\n",
    "max_len = len(dtm)\n",
    "\n",
    "# Set the column names to the words that the vectorizer found\n",
    "dtm.columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Transpose the dataframe so that the words are the rows\n",
    "term_document_matrix = dtm.T\n",
    "\n",
    "# Name the columns such that each column is a numbered document\n",
    "term_document_matrix.columns = ['Doc '+str(i) for i in range(max_len)]\n",
    "term_document_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "What's the top word used by Trump in this time period? What's the top 25 words? Also, draw a graph of these (just run .plot.bar() on a series, in a new Jupyter cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "\n",
    "Though we have already removed very common “stop words” from our analysis, it is common practice in quantitative text analysis to identify unusual words that might set one document apart from the others. The metric most commonly used to identify these unusual words is “Term Frequency Inverse Document Frequency” (tf-idf) - I haven't shown this here, but is useful to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-Based Quantitative Text Analysis\n",
    "Though word frequency counts and tf-idf can be an informative way to examine text-based data, another very popular techniques involves counting the number of words that appear in each document that have been assigned a particular meaning or value to the researcher. There are numerous examples that we shall discuss below— some of which are more sophisticated than others.\n",
    "\n",
    "### Creating your own dictionary\n",
    "\n",
    "To begin, let’s make our own dictionary of terms we want to examine from the Trump tweet dataset. Suppose we are doing a study of economic issues, and want to subset those tweets that contain words associated with the economy. To do this, we could first create a list or “dictionary” or terms that are associated with the economy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_dictionary = [\"economy\",\"unemployment\",\"trade\",\"tariffs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Subset the dataframe of tweets, such that we only store tweets which contain the words of the dictionary above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "The example above was somewhat arbitrary and mostly designed to introduce you to the concept of dictionary-base text analysis. The list of economic terms that I came up with was very ad hoc—and though the tweets identified above each mention the economy, there are probably many more tweets in our dataset that reference economic issues that do not include the words I identified.\n",
    "\n",
    "Dictionary-based approaches are often most useful when a high-quality dictionary is available that is of interest to the researcher or analyst. One popular type of dictionary is a sentiment dictionary which can be used to assess the valence of a given text by searching for words that describe affect or opinion. Some of these dictionaries are created by examining comparing text-based evaluations of products in online forums to ratings systems. Others are created via systematic observation of people writing who have been primed to write about different emotions.\n",
    "\n",
    "Pre-made dictionaries exist.\n",
    "These include the\n",
    "- `afinn` which includes a list of sentiment-laden words that appeared in Twitter discussions of climate change\n",
    "- `bing` which includes sentiemnt words identified on online forums;\n",
    "- and `nrc` which is a dictionary that was created by having workers on Amazon mechanical Turk code the emotional valence of a long list of terms.\n",
    "  \n",
    "These algorithims often produce similar results, even though they are trained on different datasets (meaning they identify sentiment laden words using different corpora). Each of these dictionaries only describe sentiment-laden words in the English language. They also have different scales.\n",
    "\n",
    "We are going to used a good pre-trained dictionary called VADER, part of the NLTK package in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\monkt\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.6908}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"murder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "This returns a dictionary with the sentiment scores of the phrase for different models (neg, neu, pos or compound.)\n",
    "\n",
    "The VADER algorithm outputs sentiment scores to 4 classes of sentiments https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L441:\n",
    "- neg: Negative\n",
    "- neu: Neutral\n",
    "- pos: Positive\n",
    "- compound: Compound (i.e. aggregated score)\n",
    "\n",
    "\n",
    " This gives you enough now to perform the main task: understand how sentiment has changed over time for Trumps tweets - in whatever way you wish. Make sure you find the sentiment for each word only, even though NLTK will allow you to do more.\n",
    " - It may be useful just to understand the number of negative words in a Tweet, as something to track over time simply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "59f3145cc67fcda0343c2852f1f97113a2e6e98841e887156424448e7071ad54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
