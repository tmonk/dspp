{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7732bd52-e317-4499-9c5c-7f0c6877a079",
   "metadata": {},
   "source": [
    "# Non-linear machine learning - trees\n",
    "#### with thanks to Google's Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d975cbb-dbba-49d2-b75b-9f6da1c64ff9",
   "metadata": {},
   "source": [
    "## Trees\n",
    "\n",
    "The model is now  the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276a1ab4-d9fb-48ed-ae2f-5dbe6a5085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "model = DecisionTreeRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b796b2-fa74-45bf-bb92-190e83a76f60",
   "metadata": {},
   "source": [
    "**Task** Re-run the house price analysis with the decision trees. Compare the in and out of sample MAE with the linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f06f973-a563-49d4-aed7-bb7416dabcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ols\n",
      "26861.096683034102\n",
      "30671.232158103503\n",
      "tree\n",
      "27.48675799086758\n",
      "32245.33698630137\n"
     ]
    }
   ],
   "source": [
    "# Solutions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "h = pd.read_csv('train.csv')\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "feature_names = [\"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\",\n",
    "                      \"FullBath\", \"BedroomAbvGr\", \"TotRmsAbvGrd\"]\n",
    "\n",
    "X=h[feature_names]\n",
    "y = h.SalePrice\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 2)\n",
    "\n",
    "\n",
    "# Define model\n",
    "h_model_ols = linear_model.LinearRegression()\n",
    "# Fit model\n",
    "h_model_ols.fit(train_X, train_y)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "print('ols')\n",
    "val_predictions = h_model_ols.predict(val_X)\n",
    "print(mean_absolute_error(h_model_ols.predict(train_X), train_y))\n",
    "print(mean_absolute_error(val_y, val_predictions))\n",
    "\n",
    "\n",
    "# Define model\n",
    "h_model_tree = DecisionTreeRegressor(random_state=1)\n",
    "# Fit model\n",
    "h_model_tree.fit(train_X, train_y)\n",
    "\n",
    "# get predicted prices on validation data\n",
    "print('tree')\n",
    "val_predictions_tree = h_model_tree.predict(val_X)\n",
    "print(mean_absolute_error(h_model_tree.predict(train_X), train_y))\n",
    "print(mean_absolute_error(val_y, val_predictions_tree))\n",
    "\n",
    "## Note the wildly better in sample performance, but worse out of sample performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184c187-9bf0-4961-a97b-373ef19a61c4",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "Experimenting With Different ModelsÂ¶\n",
    "\n",
    "Now that you have a reliable way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions. But what alternatives do you have for models?\n",
    "\n",
    "You can see in scikit-learn's documentation that the decision tree model has many options (more than you'll want or need for a long time). The most important options determine the tree's depth. A tree's depth is a measure of how many splits it makes before coming to a prediction. This is a relatively shallow tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194058fc-7159-4b11-a203-ab976dd1a158",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/R3ywQsR.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afde4e9-f3af-4644-8a0b-4ffa524691fe",
   "metadata": {},
   "source": [
    "In practice, it's not uncommon for a tree to have 10 splits between the top level (all houses) and a leaf. As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. If a tree only had 1 split, it divides the data into 2 groups. If each group is split again, we would get 4 groups of houses. Splitting each of those again would create 8 groups. If we keep doubling the number of groups by adding more splits at each level, we'll have $2^{10}$ groups of houses by the time we get to the 10th level. That's 1024 leaves.\n",
    "\n",
    "When we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n",
    "\n",
    "This is a phenomenon called overfitting, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.\n",
    "\n",
    "At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called underfitting.\n",
    "\n",
    "Since we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in the figure below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d18763-75b9-45f3-bf4d-558c20737e92",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/AXSEOfI.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c697343-f5f8-475d-bb8e-e5626c87bbd3",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "\n",
    "We can use a utility function to help compare MAE scores from different values for max_leaf_nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "683a7602-5f70-446a-89ef-562a8b21c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd81871-7948-4abe-9387-ee80f77b21ee",
   "metadata": {},
   "source": [
    "**Task** Using this function - find the optimal number of leaf nodes out of the followin options ['5','50','500','5000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67677c90-55a7-4af8-a4e0-bc24d9b6a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  40532\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  31384\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  31339\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  31469\n"
     ]
    }
   ],
   "source": [
    "#Solution\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "    \n",
    "#Of the options listed, 500 is the optimal number of leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920b745-193a-40eb-801f-23c3a7c765cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "725d83b4-2f96-4116-9e8d-fdf05db00220",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80de9a-6fb6-445c-9ca7-2f91fe440d62",
   "metadata": {},
   "source": [
    "Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n",
    "\n",
    "Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n",
    "\n",
    "The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e4387-9f33-4d6e-aa98-288050f46eb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the RandomForestRegressor class instead of DecisionTreeRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5f05c5b-9af5-4b5f-b10d-f2d5af410b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89c60e-f812-4659-8201-3337ec19ef5a",
   "metadata": {},
   "source": [
    "**Task** Does the RF improve our MAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7c0bed0-9f16-4e7a-a9e8-a11f969afc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25050.85186692759\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "forest_model.fit(train_X, train_y)\n",
    "preds_rf = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, preds_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a09ff9-cdbe-4d5b-a5e5-0347638c2b37",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\n",
    "\n",
    "It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)\n",
    "\n",
    "Then, we start the cycle:\n",
    "\n",
    "- First, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\n",
    "- These predictions are used to calculate a loss function (like mean squared error, for instance).\n",
    "- Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (Side note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.)\n",
    "- Finally, we add the new model to ensemble, and ...\n",
    "- ... repeat!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035d3ef-456e-45d3-b842-a455fa0b4d96",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/MvCGENh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7a32403-fd2b-496c-89c7-1f48a77f99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 26637.48289811644\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "my_model.fit(train_X, train_y)\n",
    "predictions = my_model.predict(val_X)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c756b7b-6191-481a-a0e2-204e02d1ec17",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "We can also train our XGBoost model, and fine tune the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c8e3ee2-1910-4853-a6bb-7cd188d56ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 26115.46934931507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/lib/python3.10/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(train_X, train_y, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(val_X, val_y)], \n",
    "             verbose=False)\n",
    "predictions = my_model.predict(val_X)\n",
    "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea97f51-2e7c-4e05-af1d-8b58238e592a",
   "metadata": {},
   "source": [
    "### Details\n",
    "XGBoost has a few parameters that can dramatically affect accuracy and training speed. The first parameters you should understand are:\n",
    "\n",
    "\n",
    "#### n_estimators\n",
    "\n",
    "n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "\n",
    "- Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n",
    "- Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about).\n",
    "\n",
    "Typical values range from 100-1000, though this depends a lot on the learning_rate parameter discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a99ef0-8c0e-4837-ab92-b9d7c93fd4de",
   "metadata": {},
   "source": [
    "\n",
    "#### early_stopping_round\n",
    "\n",
    "early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.\n",
    "\n",
    "When using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the eval_set parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6ef03-e91f-47c0-835d-d1a8be3f33c3",
   "metadata": {},
   "source": [
    "#### learning_rate\n",
    "\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.\n",
    "\n",
    "This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically.\n",
    "\n",
    "In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c283fa-f9ba-4270-a698-936e595be988",
   "metadata": {},
   "source": [
    "#### n_jobs\n",
    "\n",
    "On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f2ead-1c17-4cd3-a969-7f567004fd97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you're interested in this, I highly recommend you learn these - which build on what I've shown in class:\n",
    "- Imputation: https://www.kaggle.com/code/alexisbcook/missing-values\n",
    "- Cleaning pipelines: https://www.kaggle.com/code/alexisbcook/pipelines\n",
    "\n",
    "And then try and enter this competition!\n",
    "https://www.kaggle.com/c/home-data-for-ml-course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
